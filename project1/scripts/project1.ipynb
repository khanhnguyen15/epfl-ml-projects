{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(X):\n",
    "    x = np.copy(X)\n",
    "    \"\"\"Remove weird values from the original data set.\"\"\"\n",
    "    x[abs(x) ==  999] = np.nan\n",
    "    mean_x = np.nanmean(x, axis = 0)\n",
    "    std_x = np.nanstd(x, axis=0)\n",
    "    rows, cols = x.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if(np.isnan(x[i][j])):\n",
    "                x[i][j] = mean_x[j]\n",
    "                \n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x, mean_x, std_x):\n",
    "    \"\"\"Standardize values from the cleaned data set.\"\"\"\n",
    "    tX = np.copy(x)\n",
    "    tX = tX - mean_x[np.newaxis, :]\n",
    "    tX = tX / std_x[np.newaxis, :]\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_y(y):\n",
    "    y[y == -1.0] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = clean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "standarized_x = standardize(x, mean_x, std_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf = change_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(yf, standarized_x, seed, size_samples):\n",
    "    tX = np.copy(standarized_x)\n",
    "    y = np.copy(yf)\n",
    "    \n",
    "    y = np.expand_dims(y, axis=1)\n",
    "    np.random.seed(seed)\n",
    "    num_observations = y.shape[0]\n",
    "    random_permuted_indices = np.random.permutation(num_observations)\n",
    "    y = y[random_permuted_indices]\n",
    "    tX = tX[random_permuted_indices]\n",
    "    return y[:size_samples,:], tX[:size_samples,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_standardize(x, mean_x, std_x):\n",
    "    \"\"\"Reverse the procedure of standardization.\"\"\"\n",
    "    x = x * std_x\n",
    "    x = x + mean_x\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef calculate_mse(e):\\n    \\n    return 1/2*np.mean(e**2)\\n\\ndef compute_mae_loss(y, tx, w):\\n   \\n    e = y - tx.dot(w)\\n    return calculate_mse(e)\\n    \\ndef batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\\n    \"\"\"\\n    Generate a minibatch iterator for a dataset.\\n    Takes as input two iterables (here the output desired values \\'y\\' and the input data \\'tx\\')\\n    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\\n    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\\n    Example of use :\\n    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\\n        <DO-SOMETHING>\\n    \"\"\"\\n    data_size = len(y)\\n\\n    if shuffle:\\n        shuffle_indices = np.random.permutation(np.arange(data_size))\\n        shuffled_y = y[shuffle_indices]\\n        shuffled_tx = tx[shuffle_indices]\\n    else:\\n        shuffled_y = y\\n        shuffled_tx = tx\\n    for batch_num in range(num_batches):\\n        start_index = batch_num * batch_size\\n        end_index = min((batch_num + 1) * batch_size, data_size)\\n        if start_index != end_index:\\n            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\\n            \\ndef compute_stoch_gradient(y, tx, w):\\n    \\n    err = y - tx.dot(w)\\n    grad = -tx.T.dot(err) / len(err)\\n    return grad, err\\n\\n\\ndef least_squares_SGD(y, tx, initial_w, max_iters, gamma):\\n    \\n    w = initial_w\\n    for n_iter in range(max_iters):\\n        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1):\\n            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\\n            w = w - gamma * grad\\n            loss = compute_loss(y, tx, w)\\n\\n        print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\\n              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\\n    \\n    return (w, loss)\\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def calculate_mse(e):\n",
    "    \n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "def compute_mae_loss(y, tx, w):\n",
    "   \n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "    \n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            w = w - gamma * grad\n",
    "            loss = compute_loss(y, tx, w)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    \n",
    "    return (w, loss)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \n",
    "    expo = np.exp(-t)\n",
    "    result = 1.0/(1.0 + expo)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(tx, y, w):\n",
    "    \n",
    "    pred = sigmoid(tx @ w)\n",
    "    gradient = tx.T @ (pred - y) \n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigmoid_loss(tx, y, w):\n",
    "    \n",
    "    predictions = sigmoid(tx @ w)\n",
    "    neg_losses_per_datapoint = -(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return neg_losses_per_datapoint.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    loss = compute_sigmoid_loss(tx, y, w)\n",
    "    # ***************************************************\n",
    "    gradient = compute_gradient(tx, y, w)\n",
    "    # ***************************************************\n",
    "    w = w - gamma * gradient\n",
    "    # ***************************************************\n",
    "    return loss, w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    w = np.concatenate(([[1]], np.copy(initial_w)), axis=0)\n",
    "    # build tx\n",
    "    tX = np.c_[np.ones((y.shape[0], 1)), tx]\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tX, w, gamma)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            return (w, loss)\n",
    "    return (w, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.47534944428531\n"
     ]
    }
   ],
   "source": [
    "y, tX = sample_data(yf, standarized_x, 23, 100)\n",
    "weights, loss = logistic_regression(y, tX, 0.001* np.ones((tX.shape[1],1)), 1000, 0.001)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test, mean, std = clean(tX_test)\n",
    "tX_test = standardize(tX_test, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/sample-submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(np.squeeze(weights[1:,:]), tX_test)\n",
    "print(y_pred)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
