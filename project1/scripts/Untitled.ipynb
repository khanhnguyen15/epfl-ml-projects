{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d074f346-9b89-4b32-bcbd-07f2f628e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## task 1) least_squares_GD\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "\n",
    "def least_squares_GD(y, tx,initial_w,max_iters, gamma):\n",
    "    \"\"\" Linear regression using gradient descent\n",
    "    \"\"\"\n",
    "    # we initialize w to a zeros vector\n",
    "    w = initial_w\n",
    "    # Define parameters to store weight and loss\n",
    "    loss = 0\n",
    "    losses = []\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        gradient,err = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # store w and loss\n",
    "        losses.append(loss)\n",
    "        # update w by gradient\n",
    "        w = w - gamma * gradient\n",
    " \n",
    "    return w, losses\n",
    "\n",
    "### task 3) least squares\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"\n",
    "    apply least squares with normal linear basis\n",
    "    returns: mse loss and optimized weights\n",
    "    params: y (class labels)\n",
    "            tx (features)\n",
    "            lambda_ (regularization rate)\n",
    "    \"\"\"\n",
    "    # num datapoints\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    # calculate the gram matrix\n",
    "    gram = tx.T.dot(tx)\n",
    "    \n",
    "    # solving linear matrix equation gram * w = b \n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(gram, b)\n",
    "    \n",
    "    mse = 1 / (2 * N) * np.sum(np.square(y - tx.dot(w)))\n",
    "    return mse, w\n",
    "\n",
    "\n",
    "### task 4) ridge regression\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    apply ridge regression with L2-norm\n",
    "    returns: mse loss and optimized weights\n",
    "    params: y (class labels)\n",
    "            tx (features)\n",
    "            lambda_ (regularization rate)\n",
    "    \"\"\"\n",
    "    # num datapoints\n",
    "    N = tx.shape[0]\n",
    "    \n",
    "    # num features\n",
    "    d = tx.shape[1]\n",
    "    \n",
    "    # lambda_ap simpler notation: lampda_ap / (2 * N) = lambda\n",
    "    lambda_ap = 2 * lambda_ * N\n",
    "    i = np.identity(d)\n",
    "    \n",
    "    # solving linear matrix equation ax = b\n",
    "    # in this case ((X^T) * X + lambda_ap * I) * w = (X^T) * y\n",
    "    a = tx.T.dot(tx) + lambda_ap * i\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    \n",
    "    mse = 1 / (2 * N) * np.sum(np.square(y - tx.dot(w)))\n",
    "    return mse, w\n",
    "\n",
    "\n",
    "\n",
    "## task 6) reg_logistic_regression\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\"\"\"\n",
    "    num_samples = y.shape[0]\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    return loss, gradient\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return loss, w\n",
    "# initial_w = 0.001* np.ones((tX.shape[1],1))\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Regularized logistic regression\"\"\"\n",
    "   # we initialize it to a zeros vector\n",
    "    w = initial_w\n",
    "    \n",
    "    losses = []\n",
    "    threshold = 0.1 # 1e-18\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        w, loss = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    norm = sum(w ** 2)\n",
    "    cost = w + lambda_ * norm / (2 * np.shape(w)[0])\n",
    "\n",
    "    return w, cost\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
